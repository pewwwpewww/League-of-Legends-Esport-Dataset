{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21d1f329",
   "metadata": {},
   "source": [
    "This Notebook is aimed at trying to analyse the E-Sport games present in the dataset with the goal of estimating the causal effect of patch recency on underdog win probability "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "478f86ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import pandas as pd\n",
    "from dowhy import CausalModel\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299081ca",
   "metadata": {},
   "source": [
    "For our data we will use the game_metadata.csv and game_player_stats.csv for the explanation of each column see github readme. The data is located in the out folder in the directory of this file\n",
    "We will aggregrate the two into a dataset containing necessary info for the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9edbaee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully loaded datasets containing 74093 unique games\n"
     ]
    }
   ],
   "source": [
    "#Load datasets from csv files using pandas\n",
    "game_metadata = pd.read_csv('./out/game_metadata.csv')\n",
    "game_player_stats = pd.read_csv('./out/game_player_stats.csv')\n",
    "\n",
    "print(f\"Succesfully loaded datasets containing {len(game_metadata['gameid'].unique())} unique games\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "6eb6749f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gameid                object\n",
      "date          datetime64[ns]\n",
      "league                object\n",
      "playoffs               int64\n",
      "patch                float64\n",
      "gamelength             int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Convert the date column of game_metadata to datetime \n",
    "game_metadata['date'] = pd.to_datetime(game_metadata['date'])\n",
    "game_metadata = game_metadata.sort_values(by='date').reset_index(drop=True)\n",
    "print(game_metadata.dtypes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e86c5c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we create our aggregrated dataframe\n",
    "matches_df = game_metadata.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "eb66387c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We now want to create a dictonary where we save for each patch when the first time was it was played \n",
    "\n",
    "patch_start = matches_df.groupby('patch')['date'].min().to_dict()\n",
    "\n",
    "#Now we add that to the dataframe in the form of a column that has the days since patch as a value\n",
    "matches_df['patch_start_date'] = matches_df['patch'].map(patch_start)\n",
    "matches_df['days_since_patch'] = (matches_df['date'] - matches_df['patch_start_date']).dt.days\n",
    "\n",
    "#For now we want to drop the patch start date column but we can add it again anytime\n",
    "matches_df.drop(columns='patch_start_date', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "0102fe79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gameid                      object\n",
      "date                datetime64[ns]\n",
      "league                      object\n",
      "playoffs                     int64\n",
      "patch                      float64\n",
      "gamelength                   int64\n",
      "days_since_patch             int64\n",
      "team_a                      object\n",
      "team_b                      object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Now we want to add team_A and team_b to each game for that we first group the gameplayerstats in such a way that \n",
    "#we have only the two teams for each gameid\n",
    "teams_df = game_player_stats.groupby(['gameid', 'teamid']).first().reset_index()\n",
    "\n",
    "# Aggregate the teamid into a list per gameid \n",
    "teams_per_game = teams_df.groupby('gameid')['teamid'].agg(list).reset_index()\n",
    "\n",
    "# Assign team_a and team_b\n",
    "teams_per_game['team_a'] = teams_per_game['teamid'].apply(lambda x: x[0])\n",
    "teams_per_game['team_b'] = teams_per_game['teamid'].apply(lambda x: x[1])\n",
    "\n",
    "# Drop the list column\n",
    "teams_per_game = teams_per_game.drop(columns=['teamid'])\n",
    "\n",
    "# Merge with matches\n",
    "matches_df = matches_df.merge(teams_per_game, on='gameid', how='left')\n",
    "print(matches_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d4787627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gameid                      object\n",
      "date                datetime64[ns]\n",
      "league                      object\n",
      "playoffs                     int64\n",
      "patch                      float64\n",
      "gamelength                   int64\n",
      "days_since_patch             int64\n",
      "team_a                      object\n",
      "team_b                      object\n",
      "winner                      object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Now we want to get the winner e.g. the result of the match into the df\n",
    "#First we groupby gameids and take the first row for our result since it will always be team_A\n",
    "game_results = game_player_stats.groupby('gameid').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'winner': 'team_a' if x.iloc[0]['result'] == 1 else 'team_b'\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "#Now we merge that into our matches df\n",
    "matches_df = matches_df.merge(game_results, on='gameid', how='left')\n",
    "print(matches_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "6f4bcfa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    53610\n",
      "1    20483\n",
      "Name: recent_patch, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Add the treatment variable, e.g. whether or not there has been a patch in the last seven days \n",
    "matches_df['recent_patch'] = ((matches_df['days_since_patch']) <= 7).astype(int)\n",
    "print(matches_df['recent_patch'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a8218c",
   "metadata": {},
   "source": [
    "Now we define the method of calculating our underdog and favorite, for which we will use the default Elo Model\n",
    "We intialize our elo with 1500 for each team and update it using the following formula \n",
    "$R_{\\text{new}} = R_{\\text{old}} + K \\cdot (\\frac{1}{1 + 10^{(R_{\\text{opponent}} - R_{\\text{old}})/400}})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f3c500ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_elo(rating_A, rating_B, result, k=20):\n",
    "    outcome_a = None\n",
    "    if result == 'team_a':\n",
    "        outcome_a = 1\n",
    "    else:\n",
    "        outcome_a = 0\n",
    "    \n",
    "    expected_outcome_a = 1 / (1+10 ** ((rating_B-rating_A) / 400))\n",
    "    expected_outcome_b = 1 - expected_outcome_a\n",
    "\n",
    "    new_rating_a = rating_A + k*(outcome_a - expected_outcome_a)\n",
    "    new_rating_b = rating_B + k*((1 - outcome_a) - expected_outcome_b)\n",
    "\n",
    "    return new_rating_a, new_rating_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5edcf567",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we want to calculate the elo for every match \n",
    "initial_elo = 1500\n",
    "K = 20\n",
    "\n",
    "# Track current Elo for each team in a dict for easy access\n",
    "team_elo = {}\n",
    "\n",
    "# Pre-allocate arrays for pre-match Elo\n",
    "pre_elo_a = np.zeros(len(matches_df))\n",
    "pre_elo_b = np.zeros(len(matches_df))\n",
    "\n",
    "# Iterate through matches efficiently\n",
    "team_a_list = matches_df['team_a'].to_numpy()\n",
    "team_b_list = matches_df['team_b'].to_numpy()\n",
    "results = matches_df['winner'].to_numpy()\n",
    "\n",
    "team_streak = {}\n",
    "\n",
    "winstreak_a = np.zeros(len(matches_df))\n",
    "winstreak_b = np.zeros(len(matches_df))\n",
    "\n",
    "for i in range(len(matches_df)):\n",
    "    team_a = team_a_list[i]\n",
    "    team_b = team_b_list[i]\n",
    "    \n",
    "    # Current Elo or initialize\n",
    "    elo_a = team_elo.get(team_a, initial_elo)\n",
    "    elo_b = team_elo.get(team_b, initial_elo)\n",
    "    streak_a = team_streak.get(team_a, 0)\n",
    "    streak_b = team_streak.get(team_b, 0)\n",
    "\n",
    "    # Save pre-match Elo and winstreak\n",
    "    pre_elo_a[i] = elo_a\n",
    "    pre_elo_b[i] = elo_b\n",
    "    winstreak_a[i] = streak_a\n",
    "    winstreak_b[i] = streak_b\n",
    "\n",
    "    # Update Elo\n",
    "    new_rating_a, new_rating_b = update_elo(elo_a, elo_b, results[i], K)\n",
    "    \n",
    "    # Update dictionary\n",
    "    team_elo[team_a] = new_rating_a\n",
    "    team_elo[team_b] = new_rating_b\n",
    "\n",
    "    #update the winstreak\n",
    "    if results[i] == 'team_a':\n",
    "        team_streak[team_a] = streak_a + 1\n",
    "        team_streak[team_b] = 0\n",
    "    else:\n",
    "        team_streak[team_b] = streak_b + 1\n",
    "        team_streak[team_a] = 0\n",
    "\n",
    "# Assign back to DataFrame\n",
    "matches_df['elo_a'] = pre_elo_a\n",
    "matches_df['elo_b'] = pre_elo_b\n",
    "matches_df['winstreak_a'] = winstreak_a\n",
    "matches_df['winstreak_b'] = winstreak_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "20781b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Elo across all teams: 1612.314340095789\n",
      "gameid                      object\n",
      "date                datetime64[ns]\n",
      "league                      object\n",
      "playoffs                     int64\n",
      "patch                      float64\n",
      "gamelength                   int64\n",
      "days_since_patch             int64\n",
      "team_a                      object\n",
      "team_b                      object\n",
      "winner                      object\n",
      "recent_patch                 int64\n",
      "elo_a                      float64\n",
      "elo_b                      float64\n",
      "winstreak_a                float64\n",
      "winstreak_b                float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#For example lets print out max and min elo \n",
    "max_elo = max(team_elo.values())\n",
    "print(\"Max Elo across all teams:\", max_elo)\n",
    "\n",
    "print(matches_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "59881da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now add the remaining columns \n",
    "#Add elo diff \n",
    "matches_df['elo_diff'] = matches_df['elo_a'] - matches_df['elo_b']\n",
    "\n",
    "#Add underdog column\n",
    "matches_df['underdog'] = matches_df.apply(\n",
    "    lambda row: 'team_a' if row['elo_a'] < row['elo_b'] else 'team_b',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "#Add a column for site advantage (e.g. if the underdog is blue side)\n",
    "matches_df['underdog_side_adv'] = matches_df.apply(\n",
    "    lambda row: 1 if row['underdog'] == 'team_a' else 0,\n",
    "    axis=1\n",
    ")\n",
    "#add the outcome column or the upset column \n",
    "matches_df['upset'] = (matches_df['winner'] == matches_df['underdog']).astype(int)\n",
    "\n",
    "#save matches_df since its an interesting / useful dataframe \n",
    "matches_df.to_csv('./out/matches_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d197a68",
   "metadata": {},
   "source": [
    "Now we analyse the causal effect using dowhy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "4b8ab9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Causal effect of recent_patch on upset:\n",
      "-0.0021324551577072055\n",
      "Refute: Use a Placebo Treatment\n",
      "Estimated effect:-0.0021324551577072055\n",
      "New effect:-7.355620638926755e-05\n",
      "p value:0.94\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "treatment = 'recent_patch'  \n",
    "outcome = 'upset'         \n",
    "confounders = ['elo_diff']\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "# Create the causal model\n",
    "model = CausalModel(\n",
    "    data=matches_df,\n",
    "    treatment=treatment,\n",
    "    outcome=outcome,\n",
    "    common_causes=confounders\n",
    ")\n",
    "\n",
    "# Identify causal effect\n",
    "identified_estimand = model.identify_effect()\n",
    "\n",
    "# Estimate the effect using a linear regression\n",
    "estimate = model.estimate_effect(\n",
    "    identified_estimand,\n",
    "    method_name=\"backdoor.propensity_score_matching\"\n",
    ")\n",
    "\n",
    "# Print the effect\n",
    "print(\"Causal effect of recent_patch on upset:\")\n",
    "print(estimate.value)\n",
    "\n",
    "# Optional: Refute the estimate to check robustness\n",
    "refutation = model.refute_estimate(\n",
    "    identified_estimand,\n",
    "    estimate,\n",
    "    method_name=\"placebo_treatment_refuter\"\n",
    ")\n",
    "print(refutation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "db757210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gameid                       object\n",
      "date                 datetime64[ns]\n",
      "league                       object\n",
      "playoffs                      int64\n",
      "patch                       float64\n",
      "gamelength                    int64\n",
      "days_since_patch              int64\n",
      "team_a                       object\n",
      "team_b                       object\n",
      "winner                       object\n",
      "recent_patch                  int64\n",
      "elo_a                       float64\n",
      "elo_b                       float64\n",
      "winstreak_a                 float64\n",
      "winstreak_b                 float64\n",
      "elo_diff                    float64\n",
      "underdog                     object\n",
      "underdog_side_adv             int64\n",
      "upset                         int64\n",
      "propensity_score            float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(matches_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "1afd580d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Causal effect of underdog_side_adv on upset: 0.06733957711046917\n"
     ]
    }
   ],
   "source": [
    "# Define treatment, outcome, and confounders\n",
    "treatment = \"underdog_side_adv\"\n",
    "outcome = \"upset\"\n",
    "confounders = [\"elo_diff\", \"elo_a\", \"elo_b\", \"recent_patch\", \"playoffs\"]\n",
    "\n",
    "# Initialize the causal model\n",
    "model = CausalModel(\n",
    "    data=matches_df,\n",
    "    treatment=treatment,\n",
    "    outcome=outcome,\n",
    "    common_causes=confounders\n",
    ")\n",
    "\n",
    "# Identify the causal effect\n",
    "identified_estimand = model.identify_effect()\n",
    "\n",
    "# Estimate the causal effect using linear regression\n",
    "estimate = model.estimate_effect(\n",
    "    identified_estimand,\n",
    "    method_name=\"backdoor.linear_regression\"\n",
    ")\n",
    "print(f\"Causal effect of {treatment} on {outcome}: {estimate.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "06304414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'matches_df[\\'treatment\\'] = (matches_df[\\'winstreak_a\\'] >= 3).astype(int)  # Example for team A\\nmatches_df[\\'outcome\\'] = (matches_df[\\'winner\\'] == \\'team_a\\').astype(int)\\n\\nimport dowhy\\nfrom dowhy import CausalModel\\n\\n# Choose the variables for the model\\ntreatment = \\'treatment\\'\\noutcome = \\'outcome\\'\\nconfounders = [\\n    \\'elo_diff\\',\\n    \\'winstreak_b\\',      # opponent streak\\n    \\'playoffs\\'\\n]\\n\\n# Create a causal model\\nmodel = CausalModel(\\n    data=matches_df,\\n    treatment=treatment,\\n    outcome=outcome,\\n    common_causes=confounders\\n)\\n\\n# View the causal graph\\nmodel.view_model(layout=\"dot\")\\n\\n# Identify the causal effect\\nidentified_estimand = model.identify_effect()\\n\\n# Estimate the causal effect using linear regression as a start\\nestimate = model.estimate_effect(\\n    identified_estimand,\\n    method_name=\"backdoor.linear_regression\"\\n)\\n\\nprint(\"Causal Estimate:\", estimate.value)\\n#print(\"p-value:\", estimate.test_stat_significance[\\'p_value\\'])\\n\\n# Refute the estimate (robustness checks)\\nrefute1 = model.refute_estimate(identified_estimand, estimate, method_name=\"random_common_cause\")\\nprint(refute1)\\n\\nrefute2 = model.refute_estimate(identified_estimand, estimate, method_name=\"placebo_treatment_refuter\")\\nprint(refute2)\\n\\nrefute3 = model.refute_estimate(identified_estimand, estimate, method_name=\"data_subset_refuter\")\\nprint(refute3)'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''matches_df['treatment'] = (matches_df['winstreak_a'] >= 3).astype(int)  # Example for team A\n",
    "matches_df['outcome'] = (matches_df['winner'] == 'team_a').astype(int)\n",
    "\n",
    "import dowhy\n",
    "from dowhy import CausalModel\n",
    "\n",
    "# Choose the variables for the model\n",
    "treatment = 'treatment'\n",
    "outcome = 'outcome'\n",
    "confounders = [\n",
    "    'elo_diff',\n",
    "    'winstreak_b',      # opponent streak\n",
    "    'playoffs'\n",
    "]\n",
    "\n",
    "# Create a causal model\n",
    "model = CausalModel(\n",
    "    data=matches_df,\n",
    "    treatment=treatment,\n",
    "    outcome=outcome,\n",
    "    common_causes=confounders\n",
    ")\n",
    "\n",
    "# View the causal graph\n",
    "model.view_model(layout=\"dot\")\n",
    "\n",
    "# Identify the causal effect\n",
    "identified_estimand = model.identify_effect()\n",
    "\n",
    "# Estimate the causal effect using linear regression as a start\n",
    "estimate = model.estimate_effect(\n",
    "    identified_estimand,\n",
    "    method_name=\"backdoor.linear_regression\"\n",
    ")\n",
    "\n",
    "print(\"Causal Estimate:\", estimate.value)\n",
    "#print(\"p-value:\", estimate.test_stat_significance['p_value'])\n",
    "\n",
    "# Refute the estimate (robustness checks)\n",
    "refute1 = model.refute_estimate(identified_estimand, estimate, method_name=\"random_common_cause\")\n",
    "print(refute1)\n",
    "\n",
    "refute2 = model.refute_estimate(identified_estimand, estimate, method_name=\"placebo_treatment_refuter\")\n",
    "print(refute2)\n",
    "\n",
    "refute3 = model.refute_estimate(identified_estimand, estimate, method_name=\"data_subset_refuter\")\n",
    "print(refute3)'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
