{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "353485b2",
   "metadata": {},
   "source": [
    "In general this was meant to originally convey the effects of the visionscore on several factors but as you can see at the end I mostly analysed the effect of a gold lead at minute 15 on the impact of the result for the relative sides and also the how much that increases win probability we can see a causal effect of 0.25 for Blue side and 0.27 for red side given that we have a gold lead of minute 15 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "44159ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cace223c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gameid                object\n",
      "playerid              object\n",
      "teamid                object\n",
      "side                  object\n",
      "position              object\n",
      "champion              object\n",
      "result                 int64\n",
      "kills                  int64\n",
      "deaths                 int64\n",
      "assists                int64\n",
      "damagetochampions    float64\n",
      "visionscore          float64\n",
      "earnedgold           float64\n",
      "total cs             float64\n",
      "golddiffat15         float64\n",
      "csdiffat15           float64\n",
      "xpdiffat15           float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#import files \n",
    "game_metadata = pd.read_csv('./out/game_metadata.csv')\n",
    "game_player_stats = pd.read_csv('./out/game_player_stats.csv')\n",
    "matches_df_original = pd.read_csv('./out/matches_df.csv')\n",
    "matches_df = matches_df_original.copy()\n",
    "game_player_stats = game_player_stats[game_player_stats['visionscore'] != 0.0]\n",
    "zero_vs_count = (game_player_stats['visionscore'] == 0.0).sum()\n",
    "print(game_player_stats.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "920916cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign each team in a game to team_A or team_B\n",
    "# We'll use rank within each game to determine side\n",
    "game_player_stats = game_player_stats.copy()\n",
    "game_player_stats['team_side'] = (\n",
    "    game_player_stats.groupby('gameid')['teamid']\n",
    "    .transform(lambda x: pd.factorize(x)[0])  # 0 and 1\n",
    ")\n",
    "\n",
    "# Map 0 -> team_A, 1 -> team_B\n",
    "team_map = {0: 'team_A', 1: 'team_B'}\n",
    "game_player_stats['team_side'] = game_player_stats['team_side'].map(team_map)\n",
    "\n",
    "# Aggregate vision score & kills per team side\n",
    "team_stats = (\n",
    "    game_player_stats\n",
    "    .groupby(['gameid', 'team_side'], as_index=False)\n",
    "    .agg(\n",
    "        vision_score=('visionscore', 'sum'),\n",
    "        team_kills=('kills', 'sum')\n",
    "    )\n",
    ")\n",
    "\n",
    "# Pivot to wide format with just team_A / team_B columns\n",
    "team_stats_wide = team_stats.pivot(index='gameid', columns='team_side')\n",
    "team_stats_wide.columns = [f\"{stat}_{side}\" for stat, side in team_stats_wide.columns]\n",
    "team_stats_wide = team_stats_wide.reset_index()\n",
    "\n",
    "# Merge into matches_df\n",
    "matches_df = matches_df.merge(team_stats_wide, on='gameid', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7686875e",
   "metadata": {},
   "outputs": [],
   "source": [
    "game_player_stats['abs_golddiff_15'] = game_player_stats['golddiffat15'].abs()\n",
    "\n",
    "\n",
    "# Take one row per game (any row is fine, since it's the same for all players in the game)\n",
    "golddiff_per_game_abs = game_player_stats.drop_duplicates(subset=['gameid'])[['gameid', 'abs_golddiff_15']]\n",
    "golddiff_per_game =game_player_stats.drop_duplicates(subset=['gameid'])[['gameid', 'golddiffat15']]\n",
    "# Merge into matches_df\n",
    "matches_df = matches_df.merge(golddiff_per_game_abs, on='gameid', how='left')\n",
    "matches_df = matches_df.merge(golddiff_per_game, on='gameid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cc74dd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_df = matches_df[~matches_df['vision_score_team_A'].isnull()]\n",
    "matches_df = matches_df[~matches_df['vision_score_team_B'].isnull()]\n",
    "matches_df = matches_df[~matches_df['team_kills_team_A'].isnull()]\n",
    "matches_df = matches_df[~matches_df['team_kills_team_B'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6310e360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0               int64\n",
      "gameid                  object\n",
      "date                    object\n",
      "league                  object\n",
      "playoffs                 int64\n",
      "patch                  float64\n",
      "gamelength               int64\n",
      "days_since_patch         int64\n",
      "team_a                  object\n",
      "team_b                  object\n",
      "winner                  object\n",
      "recent_patch             int64\n",
      "elo_a                  float64\n",
      "elo_b                  float64\n",
      "winstreak_a            float64\n",
      "winstreak_b            float64\n",
      "elo_diff               float64\n",
      "underdog                object\n",
      "underdog_side_adv        int64\n",
      "upset                    int64\n",
      "vision_score_team_A    float64\n",
      "vision_score_team_B    float64\n",
      "team_kills_team_A      float64\n",
      "team_kills_team_B      float64\n",
      "abs_golddiff_15        float64\n",
      "golddiffat15           float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(matches_df.dtypes)\n",
    "matches_df['total_kills'] = matches_df['team_kills_team_A'] + matches_df['team_kills_team_B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e0840793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Causal Estimate: 1.064035882396965\n"
     ]
    }
   ],
   "source": [
    "from dowhy import CausalModel\n",
    "\n",
    "# Define DAG\n",
    "causal_graph = \"\"\"\n",
    "digraph {\n",
    "    recent_patch -> total_kills\n",
    "    elo_diff -> total_kills\n",
    "    vision_score_team_A -> total_kills\n",
    "    vision_score_team_B -> total_kills\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Build causal model directly on matches_df\n",
    "model = CausalModel(\n",
    "    data=matches_df,\n",
    "    treatment=\"recent_patch\",\n",
    "    outcome=\"total_kills\",\n",
    "    graph=causal_graph\n",
    ")\n",
    "\n",
    "# Identify effect\n",
    "identified_estimand = model.identify_effect()\n",
    "\n",
    "# Estimate causal effect using linear regression (backdoor)\n",
    "estimate = model.estimate_effect(\n",
    "    identified_estimand,\n",
    "    method_name=\"backdoor.linear_regression\"\n",
    ")\n",
    "\n",
    "print(\"Causal Estimate:\", estimate.value)\n",
    "#print(estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8589b6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0               int64\n",
      "gameid                  object\n",
      "date                    object\n",
      "league                  object\n",
      "playoffs                 int64\n",
      "patch                  float64\n",
      "gamelength               int64\n",
      "days_since_patch         int64\n",
      "team_a                  object\n",
      "team_b                  object\n",
      "winner                  object\n",
      "recent_patch             int64\n",
      "elo_a                  float64\n",
      "elo_b                  float64\n",
      "winstreak_a            float64\n",
      "winstreak_b            float64\n",
      "elo_diff               float64\n",
      "underdog                object\n",
      "underdog_side_adv        int64\n",
      "upset                    int64\n",
      "vision_score_team_A    float64\n",
      "vision_score_team_B    float64\n",
      "team_kills_team_A      float64\n",
      "team_kills_team_B      float64\n",
      "abs_golddiff_15        float64\n",
      "golddiffat15           float64\n",
      "total_kills            float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(matches_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d1379ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Causal Estimate: -0.11596670763367456\n"
     ]
    }
   ],
   "source": [
    "from dowhy import CausalModel\n",
    "\n",
    "# Define DAG\n",
    "causal_graph = \"\"\"\n",
    "digraph {\n",
    "    elo_diff -> abs_golddiff_15\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Build causal model directly on matches_df\n",
    "model = CausalModel(\n",
    "    data=matches_df,\n",
    "    treatment=\"elo_diff\",\n",
    "    outcome=\"abs_golddiff_15\",\n",
    "    graph=causal_graph\n",
    ")\n",
    "\n",
    "# Identify effect\n",
    "identified_estimand = model.identify_effect()\n",
    "\n",
    "# Estimate causal effect using linear regression (backdoor)\n",
    "estimate = model.estimate_effect(\n",
    "    identified_estimand,\n",
    "    method_name=\"backdoor.linear_regression\"\n",
    ")\n",
    "\n",
    "print(\"Causal Estimate:\", estimate.value)\n",
    "#print(estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2d09bfb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Causal Estimate: 0.8663837708127176\n"
     ]
    }
   ],
   "source": [
    "from dowhy import CausalModel\n",
    "\n",
    "# Define DAG\n",
    "causal_graph = \"\"\"\n",
    "digraph {\n",
    "    vision_score_team_A -> golddiffat15\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Build causal model directly on matches_df\n",
    "model = CausalModel(\n",
    "    data=matches_df,\n",
    "    treatment=\"vision_score_team_A\",\n",
    "    outcome=\"golddiffat15\",\n",
    "    graph=causal_graph\n",
    ")\n",
    "\n",
    "# Identify effect\n",
    "identified_estimand = model.identify_effect()\n",
    "\n",
    "# Estimate causal effect using linear regression (backdoor)\n",
    "estimate = model.estimate_effect(\n",
    "    identified_estimand,\n",
    "    method_name=\"backdoor.linear_regression\"\n",
    ")\n",
    "\n",
    "print(\"Causal Estimate:\", estimate.value)\n",
    "#print(estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8deefa83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Causal Estimate: 0.0015355752159327274\n"
     ]
    }
   ],
   "source": [
    "from dowhy import CausalModel\n",
    "\n",
    "matches_df['vision_diff'] = matches_df['vision_score_team_A']-matches_df['vision_score_team_B']\n",
    "# Define DAG\n",
    "causal_graph = \"\"\"\n",
    "digraph {\n",
    "    elo_diff -> vision_diff\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Build causal model directly on matches_df\n",
    "model = CausalModel(\n",
    "    data=matches_df,\n",
    "    treatment=\"elo_diff\",\n",
    "    outcome=\"vision_diff\",\n",
    "    graph=causal_graph\n",
    ")\n",
    "\n",
    "# Identify effect\n",
    "identified_estimand = model.identify_effect()\n",
    "\n",
    "# Estimate causal effect using linear regression (backdoor)\n",
    "estimate = model.estimate_effect(\n",
    "    identified_estimand,\n",
    "    method_name=\"backdoor.linear_regression\"\n",
    ")\n",
    "\n",
    "print(\"Causal Estimate:\", estimate.value)\n",
    "#print(estimate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1aba095",
   "metadata": {},
   "source": [
    "Interesting would be to look at how large of a gold lead it takes to have a causal effect on outcome "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c9a3a60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0               int64\n",
      "gameid                  object\n",
      "date                    object\n",
      "league                  object\n",
      "playoffs                 int64\n",
      "patch                  float64\n",
      "gamelength               int64\n",
      "days_since_patch         int64\n",
      "team_a                  object\n",
      "team_b                  object\n",
      "winner                  object\n",
      "recent_patch             int64\n",
      "elo_a                  float64\n",
      "elo_b                  float64\n",
      "winstreak_a            float64\n",
      "winstreak_b            float64\n",
      "elo_diff               float64\n",
      "underdog                object\n",
      "underdog_side_adv        int64\n",
      "upset                    int64\n",
      "vision_score_team_A    float64\n",
      "vision_score_team_B    float64\n",
      "team_kills_team_A      float64\n",
      "team_kills_team_B      float64\n",
      "abs_golddiff_15        float64\n",
      "golddiffat15           float64\n",
      "total_kills            float64\n",
      "vision_diff            float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(matches_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1465f7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0               int64\n",
      "gameid                  object\n",
      "date                    object\n",
      "league                  object\n",
      "playoffs                 int64\n",
      "patch                  float64\n",
      "gamelength               int64\n",
      "days_since_patch         int64\n",
      "team_a                  object\n",
      "team_b                  object\n",
      "winner                  object\n",
      "recent_patch             int64\n",
      "elo_a                  float64\n",
      "elo_b                  float64\n",
      "winstreak_a            float64\n",
      "winstreak_b            float64\n",
      "elo_diff               float64\n",
      "underdog                object\n",
      "underdog_side_adv        int64\n",
      "upset                    int64\n",
      "vision_score_team_A    float64\n",
      "vision_score_team_B    float64\n",
      "team_kills_team_A      float64\n",
      "team_kills_team_B      float64\n",
      "abs_golddiff_15        float64\n",
      "golddiffat15           float64\n",
      "total_kills            float64\n",
      "vision_diff            float64\n",
      "large_lead_A             int64\n",
      "large_lead_B             int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "matches_df['large_lead_A'] = (matches_df['golddiffat15'] > 1000).astype(int)\n",
    "matches_df['large_lead_B'] = (matches_df['golddiffat15'] < -1000).astype(int)\n",
    "print(matches_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "237b7314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Causal Estimate: 0.2530807371317741\n"
     ]
    }
   ],
   "source": [
    "matches_df['resultA'] = (matches_df['winner'] == 'team_a').astype(int)\n",
    "\n",
    "from dowhy import CausalModel\n",
    "\n",
    "# Define DAG\n",
    "causal_graph = \"\"\"\n",
    "digraph {\n",
    "    large_lead_A -> resultA\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Build causal model directly on matches_df\n",
    "model = CausalModel(\n",
    "    data=matches_df,\n",
    "    treatment=\"large_lead_A\",\n",
    "    outcome=\"resultA\",\n",
    "    graph=causal_graph\n",
    ")\n",
    "\n",
    "# Identify effect\n",
    "identified_estimand = model.identify_effect()\n",
    "\n",
    "# Estimate causal effect using linear regression (backdoor)\n",
    "estimate = model.estimate_effect(\n",
    "    identified_estimand,\n",
    "    method_name=\"backdoor.linear_regression\"\n",
    ")\n",
    "\n",
    "print(\"Causal Estimate:\", estimate.value)\n",
    "#print(estimate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86068db",
   "metadata": {},
   "source": [
    "This describes what happens if red side has a large gold lead and above is when blue side has "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4992f945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Causal Estimate: 0.2775275750736432\n"
     ]
    }
   ],
   "source": [
    "matches_df['resultB'] = (matches_df['winner'] != 'team_a').astype(int)\n",
    "\n",
    "from dowhy import CausalModel\n",
    "\n",
    "# Define DAG\n",
    "causal_graph = \"\"\"\n",
    "digraph {\n",
    "    large_lead_B -> resultB\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Build causal model directly on matches_df\n",
    "model = CausalModel(\n",
    "    data=matches_df,\n",
    "    treatment=\"large_lead_B\",\n",
    "    outcome=\"resultB\",\n",
    "    graph=causal_graph\n",
    ")\n",
    "\n",
    "# Identify effect\n",
    "identified_estimand = model.identify_effect()\n",
    "\n",
    "# Estimate causal effect using linear regression (backdoor)\n",
    "estimate = model.estimate_effect(\n",
    "    identified_estimand,\n",
    "    method_name=\"backdoor.linear_regression\"\n",
    ")\n",
    "\n",
    "print(\"Causal Estimate:\", estimate.value)\n",
    "#print(estimate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2af87d",
   "metadata": {},
   "source": [
    "Basic function that implements the expected outcome given the elos of team A and B so that we can look at the winrate of blue and red ahead as well as the winrate before the match "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "bf634d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elo_win_prob(rating_A, rating_B):\n",
    "    return 1 / (1 + 10 ** ((rating_B - rating_A) / 400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d845b36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We now include it into the dataframe\n",
    "matches_df['win_prob_A'] = elo_win_prob(matches_df['elo_a'], matches_df['elo_b'])\n",
    "matches_df['win_prob_B'] = 1 - matches_df['win_prob_A']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a485b586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Team A underdog: increase = 0.302\n",
      "Team B underdog: increase = 0.279\n",
      "Average underdog increase = 0.291\n"
     ]
    }
   ],
   "source": [
    "#Use this instead of the underdog column to see whether or not we have an increase \n",
    "#Could later do this but instead of doing two teams per match we map how good in general \n",
    "#win probability increases for underdogs but since outcome are realitve close \n",
    "#We can assume the mean of the two to be the outcome in general \n",
    "matches_df['underdog_A'] = matches_df['win_prob_A'] < 0.5\n",
    "matches_df['underdog_B'] = matches_df['win_prob_B'] < 0.5\n",
    "\n",
    "# Mask for underdogs with a large lead\n",
    "underdog_A_mask = (matches_df['underdog_A']) & (matches_df['large_lead_A'] == 1)\n",
    "underdog_B_mask = (matches_df['underdog_B']) & (matches_df['large_lead_B'] == 1)\n",
    "\n",
    "# Compute increase for each side\n",
    "increase_underdog_A = matches_df.loc[underdog_A_mask, 'resultA'].mean() - matches_df.loc[underdog_A_mask, 'win_prob_A'].mean()\n",
    "increase_underdog_B = matches_df.loc[underdog_B_mask, 'resultB'].mean() - matches_df.loc[underdog_B_mask, 'win_prob_B'].mean()\n",
    "\n",
    "# Aggregate both sides\n",
    "increase_underdog_avg = (increase_underdog_A + increase_underdog_B) / 2\n",
    "\n",
    "print(f\"Team A underdog: increase = {increase_underdog_A:.3f}\")\n",
    "print(f\"Team B underdog: increase = {increase_underdog_B:.3f}\")\n",
    "print(f\"Average underdog increase = {increase_underdog_avg:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e46e248b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blue ahead winrate: 0.740\n",
      "Red ahead winrate: 0.707\n"
     ]
    }
   ],
   "source": [
    "blue_mask = matches_df['large_lead_A'] == 1\n",
    "red_mask = matches_df['large_lead_B'] == 1\n",
    "\n",
    "# Compute win rates\n",
    "blue_ahead_winrate = matches_df.loc[blue_mask, 'resultA'].mean()\n",
    "red_ahead_winrate = matches_df.loc[red_mask, 'resultB'].mean()\n",
    "\n",
    "print(f\"Blue ahead winrate: {blue_ahead_winrate:.3f}\")\n",
    "print(f\"Red ahead winrate: {red_ahead_winrate:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7fca37",
   "metadata": {},
   "source": [
    "Both sides have a very large winrate for a goldlead of 1000 at minute 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8572701a",
   "metadata": {},
   "source": [
    "We find that in general we a large win probability given that we have a gold lead at minute 15 on both sides \n",
    "further we see that the causal effect of a large gold lead at minute 15 also increase the result_A by 0.25 and result_B by 0.27 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1463407d",
   "metadata": {},
   "source": [
    "Now we want to analyse whether or not and how large the effect is on the win probability of each team "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "889decd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Team A: avg increase in win probability with large lead = 0.222\n",
      "Team B: avg increase in win probability with large lead = 0.224\n"
     ]
    }
   ],
   "source": [
    "# Team A: average win probability increase if ahead\n",
    "win_prob_increase_A = matches_df.loc[matches_df['large_lead_A'] == 1, 'resultA'].mean() - matches_df['win_prob_A'].mean()\n",
    "\n",
    "# Team B: average win probability increase if ahead\n",
    "win_prob_increase_B = matches_df.loc[matches_df['large_lead_B'] == 1, 'resultB'].mean() - matches_df['win_prob_B'].mean()\n",
    "\n",
    "print(f\"Team A: avg increase in win probability with large lead = {win_prob_increase_A:.3f}\")\n",
    "print(f\"Team B: avg increase in win probability with large lead = {win_prob_increase_B:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aee6955",
   "metadata": {},
   "source": [
    "On average our win probability for both sides win prob increases by 0.22 given a gold lead of 1000 at minute 15 and if we look at the underdogs we get an increase of about 0.29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b3fe5e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0               int64\n",
      "gameid                  object\n",
      "date                    object\n",
      "league                  object\n",
      "playoffs                 int64\n",
      "patch                  float64\n",
      "gamelength               int64\n",
      "days_since_patch         int64\n",
      "team_a                  object\n",
      "team_b                  object\n",
      "winner                  object\n",
      "recent_patch             int64\n",
      "elo_a                  float64\n",
      "elo_b                  float64\n",
      "winstreak_a            float64\n",
      "winstreak_b            float64\n",
      "elo_diff               float64\n",
      "underdog                object\n",
      "underdog_side_adv        int64\n",
      "upset                    int64\n",
      "vision_score_team_A    float64\n",
      "vision_score_team_B    float64\n",
      "team_kills_team_A      float64\n",
      "team_kills_team_B      float64\n",
      "abs_golddiff_15        float64\n",
      "golddiffat15           float64\n",
      "total_kills            float64\n",
      "vision_diff            float64\n",
      "large_lead_A             int64\n",
      "large_lead_B             int64\n",
      "resultA                  int64\n",
      "resultB                  int64\n",
      "win_prob_A             float64\n",
      "win_prob_B             float64\n",
      "underdog_A                bool\n",
      "underdog_B                bool\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(matches_df.dtypes)\n",
    "matches_df.to_csv('./out/matches_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a76ae30",
   "metadata": {},
   "source": [
    "We now want to visualise the causal effect of gold lead on each sides result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b8515bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0               int64\n",
      "gameid                  object\n",
      "date                    object\n",
      "league                  object\n",
      "playoffs                 int64\n",
      "patch                  float64\n",
      "gamelength               int64\n",
      "days_since_patch         int64\n",
      "team_a                  object\n",
      "team_b                  object\n",
      "winner                  object\n",
      "recent_patch             int64\n",
      "elo_a                  float64\n",
      "elo_b                  float64\n",
      "winstreak_a            float64\n",
      "winstreak_b            float64\n",
      "elo_diff               float64\n",
      "underdog                object\n",
      "underdog_side_adv        int64\n",
      "upset                    int64\n",
      "vision_score_team_A    float64\n",
      "vision_score_team_B    float64\n",
      "team_kills_team_A      float64\n",
      "team_kills_team_B      float64\n",
      "abs_golddiff_15        float64\n",
      "golddiffat15           float64\n",
      "total_kills            float64\n",
      "vision_diff            float64\n",
      "large_lead_A             int64\n",
      "large_lead_B             int64\n",
      "resultA                  int64\n",
      "resultB                  int64\n",
      "win_prob_A             float64\n",
      "win_prob_B             float64\n",
      "underdog_A                bool\n",
      "underdog_B                bool\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(matches_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "97ebe331",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.arange(-4000, 4001, 500)  \n",
    "matches_df['gold_bin'] = pd.cut(matches_df['golddiffat15'], bins=bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d1425461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_effect_per_bin(df, treatment_col, outcome_col, confounders=['elo_diff']):\n",
    "    effects = []\n",
    "    for b in df['gold_bin'].cat.categories:\n",
    "        subset = df[df['gold_bin'] == b]\n",
    "        if subset[treatment_col].nunique() < 2:\n",
    "            continue  # skip bins without variation in treatment\n",
    "        model = CausalModel(\n",
    "            data=subset,\n",
    "            treatment=treatment_col,\n",
    "            outcome=outcome_col,\n",
    "            common_causes=confounders\n",
    "        )\n",
    "        identified_estimand = model.identify_effect()\n",
    "        estimate = model.estimate_effect(identified_estimand, method_name=\"backdoor.linear_regression\")\n",
    "        effects.append((b.mid, estimate.value))  # use mid-point of bin\n",
    "    return pd.DataFrame(effects, columns=['gold_mid', 'effect'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "074ab6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the causal effect for A and for B for each bin \n",
    "effects_A = causal_effect_per_bin(matches_df, 'large_lead_A', 'resultA')\n",
    "effects_B = causal_effect_per_bin(matches_df, 'large_lead_B', 'resultB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb9034d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(effects_A['gold_mid'], effects_A['effect'], marker='o', label='Blue Side (Team A)')\n",
    "plt.plot(effects_B['gold_mid'], effects_B['effect'], marker='s', label='Red Side (Team B)')\n",
    "plt.axhline(0, color='gray', linestyle='--')\n",
    "plt.xlabel('Gold Lead at 15 min')\n",
    "plt.ylabel('Increase in Win Probability')\n",
    "plt.title('Causal Effect of Gold Lead on Win Probability')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
